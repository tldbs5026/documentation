{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0HRwW+BQ0pEHRGTbStSG3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["코드 참고 : https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Style_Transfer_Tutorial.ipynb\n","\n","원본 : https://rn-unison.github.io/articulos/style_transfer.pdf"],"metadata":{"id":"aZ6Nvp8rurxt"}},{"cell_type":"markdown","source":["딥러닝을 활용한 스타일 전송 방법을 처음으로 제안\n","- 이미지 자체를 업데이트하는 이미지 최적화 기법"],"metadata":{"id":"5KNK4mLLu1mz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuEB4A20unc5"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","\n","import PIL\n","import matplotlib.pyplot as plt\n","\n","import copy"]},{"cell_type":"code","source":["#GPU 장치 사용\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"EtNV73pevKVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#이미지를 불러와 다운받아 Tensor 객체로 변환\n","\n","def imgage_loader(img_path, imgsize) :\n","  loader=transforms.Compose([\n","      transforms.Resize(imgsize)\n","      transforms.ToTensor()\n","  ])\n","\n","  image = PIL.Image.open(img_path)\n","  #네트워크 입력에 들어갈 이미지에 배치 목적의 차원 추가\n","  image=loader(image).unsqueeze(0)\n","  return image.to(device, torch.float)    #gpu로 올리기\n","\n","#torch.Tensor 형태의 이미지를 화면에 출력\n","def imshow(tensor) :\n","  #matplotlib는 cpu기반임으로 cpu로 옮기기\n","  image = tensor.cpu().clone()\n","  #torch.Tensor에서 사용되는 배치 목적의 차원 제거\n","  image = image.squeeze(0)\n","  #PIL객체로 변경\n","  image = transforms.ToPILImage(image)\n","  #이미지를 화면에 출력\n","  plt.imshow(image)\n","  plt.show()"],"metadata":{"id":"vGh3jQxDvQj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#저장소의 소스코드를 다운로드 하여 실습용 이미지 준비\n","!git clone https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice\n","%cd Deep-Learning-Paper-Review-and-Practice"],"metadata":{"id":"9ki3e8x6wGAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Image Reconstruction\n","\n","- 이미지를 최적화 한다는 것은 특정 손실 값을 낮추는 방향으로 이미지를 업데이트\n","- MSE Loss를 이용하여 임의의 noise를 특정한 이미지로 변환"],"metadata":{"id":"hLxDwSQ6wLue"}},{"cell_type":"code","source":["#목표 이미지 불러오기\n","img_path = './code_practices/images/cat.jpg'\n","target_image = image_loader(img_path, (512,512))\n","imshow(target_image)"],"metadata":{"id":"tTf94K-XwK5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#동일한 크기의 노이즈 이미지 준비\n","noise=torch.empty_like(target_image).uniform_(0,1).to(device)\n","imshow(noise)"],"metadata":{"id":"c1abzJ42wqWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#이미지 업데이트\n","\n","loss = nn.MSELoss()\n","iters = 100\n","lr =  1e4\n","\n","print('[Start]')\n","imshow(noise)\n","\n","for i in range(iters) :\n","  #required_grad 속성의 값을 True로 설정하여 해당 torch.Tensor의 연산을 추적\n","  noise.requires_grad = True\n","\n","  #손실 함수에 대해 미분하여 gradient 계산\n","  output = loss(noise, target_image)\n","  output.backward()\n","\n","  #계산된 기울기를 이용하여 손실 함수가 감소하는 방향으로 업데이트\n","  gradient = lr*noise.grad\n","\n","  #결과적으로 노이즈의 각 픽셀의 값이 [-eps,eps] 사이의 값이 되도록 자르기\n","  noise = torch.clamp(noise - grandient, min=0, max=1).detach()   #연산을 추적하는 것을 중단하기 위한 detach함수\n","\n","  if (i+1) % 10 == 0 :\n","    print(f'[ step : {i+1}]')\n","    print(f'Loss : {output}')\n","    imshow(noise)"],"metadata":{"id":"Kk3xUf1cwzNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이미지 불러오기"],"metadata":{"id":"Zeg4pBKwwg4X"}},{"cell_type":"code","source":["#content 이미지와 style 이미지 준비\n","content_img = image_loader('./code_practices/images/content_img_1.jpg', (512, 640))\n","style_img = image_loader('./code_practices/images/style_img_1.jpg', (512, 640))\n","\n","print('[Content image]')\n","imshow(content_img)\n","\n","print('[Style image]')\n","imshow(style_img)"],"metadata":{"id":"JX2vFsuvya7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CNN 네트워크 불러오기\n","- VGG 네트워크 사용"],"metadata":{"id":"-7IfcJ0-yqtY"}},{"cell_type":"code","source":["#뉴럴 네트워크 모델을 불러오기\n","cnn = models.vgg19(pretrained=True).features.to(device).eval()    #evaluation의 목적은 분류가 아닌 features를 추출하기 위함\n","print(cnn)"],"metadata":{"id":"aKQqMEZpyt-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#입력 정규화를 위한 초기화\n","cnn_normalization_mean = torch.tensor([0.485,0.456,0.406]).to(device)\n","cnn_normalization_std = torch.tensor([0.229,0.224,0.225]).to(device)\n","\n","class Normalization(nn.Module) :\n","  def __init__(self.mean, std) :\n","    super(Normalization, self).__init__()\n","    self.mean = mean.clone().view(-1,1,1)\n","    self.std = std.clone().view(-1,1,1)\n","\n","  def forward(self, img) :\n","    return (img-self.mean) / self.std"],"metadata":{"id":"dudaw_O0y1RB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Style Reconstruction\n","- 임의의 noise가 특정한 이미지를 스타일을 갖도록 변환\n","- style_layer 리스트 변수의 값을 조절하여 어떤 레이어를 이용할지 설정 가능\n"],"metadata":{"id":"q2ci9xUNzPgY"}},{"cell_type":"code","source":["def gram_matrix(input) :\n","  #a는 배치의 크기, b는 특징 맵의 개수, (c,d)는 특징 맵의 차원\n","  a,b,c,d = input.size()\n","  features = input.view(a*b,c*d)\n","  # torch.mm :  matrix multiplication\n","  G = torch.mm(features, features.t())     #자기 자신과 전치를 행렬곱을 하게되면 gram matrix와 동일한 값이 출력됨 \n","  return G.div(a*b*c*d)\n","\n","#Style loss계산을 위한 class 정의\n","class StyleLoss(nn.Module) :\n","  def __init__(self, target_feature) :\n","    super(StyleLoss, self).__init__()\n","    self.target = gram_matrix(target_feature).detach()\n","\n","  def forward(self, input) :\n","    G = gram_matrix(input)\n","    self.loss = F.mse_loss(G, self.target)\n","    return input\n","  #style의 gram matrix를 구한 후, 나중에 들어온 input feature의 gram matrix를 구한 후 그 둘의 mse_loss"],"metadata":{"id":"nIMXTMcjzYhi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n","\n","# 스타일 손실(style loss)을 계산하는 함수\n","def get_style_losses(cnn, style_img, noise_image):\n","    cnn = copy.deepcopy(cnn)\n","    normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n","    style_losses = []\n","    \n","    # 가장 먼저 입력 이미지가 입력 정규화(input normalization)를 수행하도록\n","    model = nn.Sequential(normalization)\n","\n","    # 현재 CNN 모델에 포함되어 있는 모든 레이어를 확인하며\n","    i = 0\n","    for layer in cnn.children():\n","        if isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = 'conv_{}'.format(i)\n","        elif isinstance(layer, nn.ReLU):\n","            name = 'relu_{}'.format(i)\n","            layer = nn.ReLU(inplace=False)\n","        elif isinstance(layer, nn.MaxPool2d):\n","            name = 'pool_{}'.format(i)\n","        elif isinstance(layer, nn.BatchNorm2d):\n","            name = 'bn_{}'.format(i)\n","        else:\n","            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n","\n","        model.add_module(name, layer)\n","\n","        # 설정한 style layer까지의 결과를 이용해 style loss를 계산\n","        if name in style_layers:\n","            target_feature = model(style_img).detach()\n","            style_loss = StyleLoss(target_feature)\n","            model.add_module(\"style_loss_{}\".format(i), style_loss)\n","            style_losses.append(style_loss)\n","\n","    # 마지막 style loss 이후의 레이어는 사용하지 않도록\n","    for i in range(len(model) - 1, -1, -1):\n","        if isinstance(model[i], StyleLoss):\n","            break\n","\n","    model = model[:(i + 1)]\n","    return model, style_losses"],"metadata":{"id":"G4021LHj0I15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def style_reconstruction(cnn, style_img, input_img, iters):\n","    model, style_losses = get_style_losses(cnn, style_img, input_img)\n","    optimizer = optim.LBFGS([input_img.requires_grad_()])   #현재 가장 많이 사용되는 optimizer는 Adam\n","\n","    print(\"[ Start ]\")\n","    imshow(input_img)\n","\n","    # 하나의 값만 이용하기 위해 배열 형태로 사용\n","    run = [0]\n","    while run[0] <= iters:\n","\n","        def closure():\n","            input_img.data.clamp_(0, 1)   #범위 제한\n","\n","            optimizer.zero_grad()   #gradient 초기화\n","            model(input_img)\n","            style_score = 0\n","\n","            for sl in style_losses:\n","                style_score += sl.loss\n","\n","            style_score *= 1e6\n","            style_score.backward()\n","\n","            run[0] += 1\n","            if run[0] % 50 == 0:\n","                print(f\"[ Step: {run[0]} / Style loss: {style_score.item()}]\")\n","                imshow(input_img)\n","            \n","            return style_score\n","        \n","        optimizer.step(closure)\n","\n","    # 결과적으로 이미지의 각 픽셀의 값이 [0, 1] 사이의 값이 되도록 자르기\n","    input_img.data.clamp_(0, 1)\n","\n","    return input_img"],"metadata":{"id":"LZqVwuDU0o1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 콘텐츠 이미지와 동일한 크기의 노이즈 이미지 준비하기\n","input_img = torch.empty_like(content_img).uniform_(0, 1).to(device)\n","imshow(input_img)"],"metadata":{"id":"AyakrcHq0xu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# style reconstruction 수행\n","output = style_reconstruction(cnn, style_img=style_img, input_img=input_img, iters=300)"],"metadata":{"id":"xZNDOe2i0zaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Content Reconstruction\n","\n","임의의 노이즈가 특정한 이미지의 콘텐츠를 가지도록 변환"],"metadata":{"id":"MU5I8q1f00kR"}},{"cell_type":"code","source":["# 콘텐츠 손실(content loss) 계산을 위한 클래스 정의\n","class ContentLoss(nn.Module):\n","    def __init__(self, target,):\n","        super(ContentLoss, self).__init__()\n","        self.target = target.detach()\n","\n","    def forward(self, input):\n","        self.loss = F.mse_loss(input, self.target)\n","        return input"],"metadata":{"id":"CYXUxxSv05Lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#논문은 4번째 레이어와 input의 loss를 확인\n","content_layers = ['conv_4']\n","\n","# 콘텐츠 손실(content loss)을 계산하는 함수\n","def get_content_losses(cnn, content_img, noise_image):\n","    cnn = copy.deepcopy(cnn)\n","    normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n","    content_losses = []\n","    \n","    # 가장 먼저 입력 이미지가 입력 정규화(input normalization)를 수행하도록\n","    model = nn.Sequential(normalization)\n","\n","    # 현재 CNN 모델에 포함되어 있는 모든 레이어를 확인하며\n","    i = 0\n","    for layer in cnn.children():\n","        if isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = 'conv_{}'.format(i)\n","        elif isinstance(layer, nn.ReLU):\n","            name = 'relu_{}'.format(i)\n","            layer = nn.ReLU(inplace=False)\n","        elif isinstance(layer, nn.MaxPool2d):\n","            name = 'pool_{}'.format(i)\n","        elif isinstance(layer, nn.BatchNorm2d):\n","            name = 'bn_{}'.format(i)\n","        else:\n","            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n","\n","        model.add_module(name, layer)\n","\n","        # 설정한 content layer까지의 결과를 이용해 content loss를 계산\n","        if name in content_layers:\n","            target_feature = model(content_img).detach()\n","            content_loss = ContentLoss(target_feature)\n","            model.add_module(\"content_loss_{}\".format(i), content_loss)\n","            content_losses.append(content_loss)\n","\n","    # 마지막 content loss 이후의 레이어는 사용하지 않도록\n","    for i in range(len(model) - 1, -1, -1):\n","        if isinstance(model[i], ContentLoss):\n","            break\n","\n","    model = model[:(i + 1)]\n","    return model, content_losses"],"metadata":{"id":"QrEjTKG90_j4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def content_reconstruction(cnn, content_img, input_img, iters):\n","    model, content_losses = get_content_losses(cnn, content_img, input_img)\n","    optimizer = optim.LBFGS([input_img.requires_grad_()])\n","\n","    print(\"[ Start ]\")\n","    imshow(input_img)\n","\n","    # 하나의 값만 이용하기 위해 배열 형태로 사용\n","    run = [0]\n","    while run[0] <= iters:\n","\n","        def closure():\n","            input_img.data.clamp_(0, 1)\n","\n","            optimizer.zero_grad()\n","            model(input_img)\n","            content_score = 0\n","\n","            for cl in content_losses:\n","                content_score += cl.loss\n","\n","            content_score.backward()\n","\n","            run[0] += 1\n","            if run[0] % 50 == 0:\n","                print(f\"[ Step: {run[0]} / Content loss: {content_score.item()}]\")\n","                imshow(input_img)\n","            \n","            return content_score\n","        \n","        optimizer.step(closure)\n","\n","    # 결과적으로 이미지의 각 픽셀의 값이 [0, 1] 사이의 값이 되도록 자르기\n","    input_img.data.clamp_(0, 1)\n","\n","    return input_img"],"metadata":{"id":"6uiiFHi11QIX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 콘텐츠 이미지와 동일한 크기의 노이즈 이미지 준비하기\n","input_img = torch.empty_like(content_img).uniform_(0, 1).to(device)\n","imshow(input_img)"],"metadata":{"id":"mvNWBXUr1dVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# content reconstruction 수행\n","output = content_reconstruction(cnn, content_img=content_img, input_img=input_img, iters=300)"],"metadata":{"id":"hxKAsea01i0R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Style Transfer\n","- content Loss와 Style Loss를 함께 이용하여 Style Transfer 진행"],"metadata":{"id":"3ciOCdes0_FW"}},{"cell_type":"code","source":["content_layers = ['conv_4']\n","style_layers = ['conv_1', 'conv_3', 'conv_5', 'conv_7', 'conv_9']\n","\n","# Style Transfer 손실(loss)을 계산하는 함수\n","def get_losses(cnn, content_img, style_img, noise_image):\n","    cnn = copy.deepcopy(cnn)\n","    normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n","    content_losses = []\n","    style_losses = []\n","    \n","    # 가장 먼저 입력 이미지가 입력 정규화(input normalization)를 수행하도록\n","    model = nn.Sequential(normalization)\n","\n","    # 현재 CNN 모델에 포함되어 있는 모든 레이어를 확인하며\n","    i = 0\n","    for layer in cnn.children():\n","        if isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = 'conv_{}'.format(i)\n","        elif isinstance(layer, nn.ReLU):\n","            name = 'relu_{}'.format(i)\n","            layer = nn.ReLU(inplace=False)\n","        elif isinstance(layer, nn.MaxPool2d):\n","            name = 'pool_{}'.format(i)\n","        elif isinstance(layer, nn.BatchNorm2d):\n","            name = 'bn_{}'.format(i)\n","        else:\n","            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n","\n","        model.add_module(name, layer)\n","\n","    #content와 style loss 모두 다 초기화 : 본문에서 동시에 업데이트하기 때문\n","    \n","        # 설정한 content layer까지의 결과를 이용해 content loss를 계산\n","        if name in content_layers:\n","            target_feature = model(content_img).detach()\n","            content_loss = ContentLoss(target_feature)\n","            model.add_module(\"content_loss_{}\".format(i), content_loss)\n","            content_losses.append(content_loss)\n","\n","        # 설정한 style layer까지의 결과를 이용해 style loss를 계산\n","        if name in style_layers:\n","            target_feature = model(style_img).detach()\n","            style_loss = StyleLoss(target_feature)\n","            model.add_module(\"style_loss_{}\".format(i), style_loss)\n","            style_losses.append(style_loss)\n","\n","    # 마지막 loss 이후의 레이어는 사용하지 않도록\n","    for i in range(len(model) - 1, -1, -1):\n","        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n","            break\n","\n","    model = model[:(i + 1)]\n","    return model, content_losses, style_losses"],"metadata":{"id":"yP-CvmkE1oYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def style_transfer(cnn, content_img, style_img, input_img, iters):\n","    model, content_losses, style_losses = get_losses(cnn, content_img, style_img, input_img)\n","    optimizer = optim.LBFGS([input_img.requires_grad_()])\n","\n","    print(\"[ Start ]\")\n","    imshow(input_img)\n","\n","    # 하나의 값만 이용하기 위해 배열 형태로 사용\n","    run = [0]\n","    while run[0] <= iters:\n","\n","        def closure():\n","            input_img.data.clamp_(0, 1)\n","\n","            optimizer.zero_grad()\n","            model(input_img)\n","            content_score = 0\n","            style_score = 0\n","\n","            for cl in content_losses:\n","                content_score += cl.loss\n","            for sl in style_losses:\n","                style_score += sl.loss\n","\n","            style_score *= 1e5\n","            loss = content_score + style_score      #total loss\n","            loss.backward()\n","\n","            run[0] += 1\n","            if run[0] % 100 == 0:\n","                print(f\"[ Step: {run[0]} / Content loss: {content_score.item()} / Style loss: {style_score.item()}]\")\n","                imshow(input_img)\n","            \n","            return content_score + style_score\n","        \n","        optimizer.step(closure)\n","\n","    # 결과적으로 이미지의 각 픽셀의 값이 [0, 1] 사이의 값이 되도록 자르기\n","    input_img.data.clamp_(0, 1)\n","\n","    return input_img"],"metadata":{"id":"qWiwevFe18Oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 콘텐츠(Content) 이미지와 스타일(Style) 이미지를 모두 준비합니다.\n","content_img = image_loader('./code_practices/images/content_img_1.jpg', (512, 640))\n","style_img = image_loader('./code_practices/images/style_img_1.jpg', (512, 640))\n","\n","print(\"[ Content Image ]\")\n","imshow(content_img)\n","print(\"[ Style Image ]\")\n","imshow(style_img)"],"metadata":{"id":"hYVQEAb_199x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 콘텐츠 이미지와 동일한 크기의 노이즈 이미지 준비하기\n","input_img = torch.empty_like(content_img).uniform_(0, 1).to(device)\n","imshow(input_img)"],"metadata":{"id":"HxcbVRVO1_6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# style transfer 수행\n","output = style_transfer(cnn, content_img=content_img, style_img=style_img, input_img=input_img, iters=900)"],"metadata":{"id":"jhbwHHcy2B1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.utils import save_image\n","\n","save_image(output.cpu().detach()[0], 'output_1.png')\n","print('이미지 파일 저장을 완료했습니다.')"],"metadata":{"id":"jP81WHNg2DoA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","# 완성된 이미지를 다운로드합니다.\n","files.download('output_1.png')"],"metadata":{"id":"prSTHrrS2Ejs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image\n","\n","Image('output_1.png')"],"metadata":{"id":"wm5JGOpa2FVv"},"execution_count":null,"outputs":[]}]}