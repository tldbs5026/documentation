{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNn+vIQTeOnRVIbc7KdXrap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790#TensorFlow-Transformer-Starter---LB-0.790\n","\n","time series eda : https://www.kaggle.com/code/cdeotte/time-series-eda#Customer-Time-Series-EDA"],"metadata":{"id":"sQIg-Mq3ox-i"}},{"cell_type":"markdown","source":["Using a transformer requires 3D data(whereas Kaggle provideds 2d data as a csv.). The shape of the data is (#customers, 13, 188) which is (batchsize, sequence legnth, feature legnth). Each customer is a time series with 13 credit card statements. And each statemenr has 188 features. The data was created and saved to numpy files. "],"metadata":{"id":"47P7E7GoWuT6"}},{"cell_type":"markdown","source":["Load Libraries"],"metadata":{"id":"TfEBgFf_dq3x"}},{"cell_type":"code","source":["import cupy, cudf\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt, gc, os\n","\n","path_to_data = '../input/amax-data-for-transformers-and-rnns/data/'\n","\n","train_model = True\n","path_to_model = './model/'\n","\n","infer_Test = True"],"metadata":{"id":"vbRllXnwdqsE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Build Transformer Model"],"metadata":{"id":"vCFjX6UYeA-J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"po_Hnyq7WruF"},"outputs":[],"source":["os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'  #tf will not use all memory\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as k\n","\n","from tensorflow import Keras\n","from tensorflow.kears import layers\n","\n","print('using tensforflow version', tf.__version__)\n"]},{"cell_type":"code","source":["class TransformerBlock(layers.Layer) :\n","  def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate=0.1) :\n","\n","    super(TransformerBlock,self).__init__()\n","    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","        [layers.Dense(ff_dim, activation='relu', ),\n","         layers.Dense(feat_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","    self.Dropout1 = layers.Dropout(rate)\n","    self.Dropout2 = layers,Dropout(rate)\n","\n","  def call(self, inputs, training) :\n","    attn_output = self.att(inputs, inputs)\n","    attn_output = self.Dropout1(attn_output, training_training)\n","\n","    out1 = self.layernorm1(inputs + attn_output)\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training_training)\n","    return self.layernorm2(out1 + ffn_output)"],"metadata":{"id":"Ce-zqb8geYQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feat_dim = 188\n","embed_dim =64\n","num_heads = 4\n","ff_dim = 128\n","\n","dropout_rate = 0.3\n","num_blocks =2\n","\n","def build_model() :\n","\n","  #input embedding layer\n","  inp = layer.Input(shape=(13,188))\n","  embeddings = []\n","\n","  for k in range(11) :\n","    emb = layers.Embedding(10,4)\n","    embeddings.append(emd(inp[:,:,:k]))\n","\n","  x = layers.Concatenate()([inp[:,:,11:]] + embeddings)\n","  x = layers.Dense(feat_dim)(x)\n","\n","  #Transformer blocks\n","  for k in range(num_blocks) :\n","    x_old = x\n","    transformer_block = TrnasformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)\n","    x = transofrmer_block(x)\n","    x = 0.9*x + 0.1*x_old   #skip connection\n","\n","\n","  x = layers.Dense(64, activation='relu')(x[:,-1,:])\n","  x = layers.Dense(32, activation='relu')(x)\n","  \n","  outputs = layers.Dense(1, activation='sigmoid')(x)\n","\n","  model = keras.Model(inputs = inp, outputs=outputs)\n","  opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n","  loss = tf.keras.losses.BinaryCrossentropy()\n","  model.compile(loss=loss, optimizer=opt)\n","  # or\n","  model.compile(loss = Binary_crossentropy, optimizer='adam')\n","  \n","  return model"],"metadata":{"id":"m-cjRA6mh0lX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Define Learning Schedule"],"metadata":{"id":"26eaLfB9jDVy"}},{"cell_type":"code","source":["import math\n","lr_start = 1e-6\n","lr_max = 1e-3\n","lr_min = 1e-6\n","lr_rampup_epochs =0\n","lr_sustrain_epochs = 0\n","epochs = 10\n","\n","def lrfn(epoch) :\n","  if epoch < lr_rampup_epochs :\n","    lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n","  elif epoch <lr_rampup_epochs + lr_sustain_epochs :\n","    lr = lr_max\n","  else :\n","    decay_total_epochs = epochs - lr_rampup_epochs - lr_sustrain_epochs -1\n","    decay_total_index = epoch - lr_rampp_epochs - lr_sustrain_epochs\n","\n","    phase=  math.pi * decat_epoch_index / decay_total+epochs\n","    cosine_decay = 0.5 * (1 + math.cos(phase))\n","\n","    lr = (lr_max - lr_min) * cosine_decay + lr_min\n","\n","  return lr\n","\n","rng = [i for i in range(epochs)]\n","lr_y = [lrfn(x) for x in rng]\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(rng, lr_y, '-o')\n","plt.xlabel('epoch') ; plt.ylabel('lr')\n","\n","print('learning rate schedule : {:.3g} to {:.3g} to {:.3g}'.format(lr_y[0], max(lr_y), ly_y[-1]))\n","\n","lr = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"],"metadata":{"id":"JdLmK8TQjCVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Define Competition Metric"],"metadata":{"id":"ylYCaco-lvVr"}},{"cell_type":"code","source":["def amex_metric_mod(y_true, y_hat) :\n","\n","  labels = np.transpose(np.array([y_true, y_hat]))\n","  labels = labels[labels[:,1].argsort()[::-1]]\n","\n","  weights = np.where(labels[:,0] == 0, 20, 1)\n","\n","  cut_vals = labels[np.cumsum(weights) <= int(0.04*np.sum(weights))]\n","\n","  top_four = np.sum(cut_vals[:,0] / np.sum(labels[:,0]))\n","\n","  gini = [0,0]\n","\n","  for i in [1,0] :\n","    labels = labels\n","    weight_random = np.cumsum(weight / np.sum(weight))\n","\n","    total_pos = np.sum(labels[:,0] * weight)\n","    cum_pos_found = np.cumsum(labels[:,0] * weight)\n","    lorentz = cum_pos_found / total_pos\n","\n","    gini[i] = np.sum((lorentz - weight_random) * weight)\n","\n","  return 0.5 * (gini[1]/gini[0] + top_four)"],"metadata":{"id":"2gfUDNLEluCY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train Model"],"metadata":{"id":"mgcxUFx0mvBV"}},{"cell_type":"code","source":["if train_model :\n","  # save true and oof\n","\n","  true = np.array([])\n","  oof = np.array([])\n","\n","  verbose = 2\n","\n","  for fold in range(5) :\n","\n","    #in dices of train and valid folds\n","    valid_idx =  [2*fold+1, 2*fold+2]\n","    train_idx = [x for x in [i for i in range(1,11) if x not in valid_idx]]\n","\n","    print('===='*25)\n","\n","    # read train data from disk\n","    x_train = []\n","    y_train = []\n","\n","    for k in train_idx :\n","      x_train.append(np.load(f'{path_to_data}data_{k}.npy'))\n","      y_trainappend(np.read_parquet(f'{path_to_data}target_{k}.pqt'))\n","\n","      x_train = np.concatenate(x_train, axis=0)\n","      y_train = np.concat(y_train).target.values\n","\n","    # read valid data from disk\n","    x_valid = []\n","    y_valid = []\n","\n","    for k in train_idx :\n","      x_valid.append(np.load(f'{path_to_data}data_{k}.npy'))\n","      y_valid.append(np.read_parquet(f'{path_to_data}target_{k}.pqt'))\n","\n","      x_valid = np.concatenate(x_valid, axis=0)\n","      y_valid = np.concat(y_valid).target.values\n","\n","\n","    # build and train model\n","\n","    k.clear_session()\n","\n","    model = build_model()\n","\n","    h = model.fit(x_train, y_train, \n","                  validation_data = (x_valid, y_valid),\n","                  batch_size = 512, epochs = epochs, verbose=verbose,\n","                  callbacks = [lr])\n","    \n","    if not os.path.exist(path_to_model) : os.makedirs(path_to_model)\n","\n","    model.save_weights(f'{path_to_model}transformer_fold_{fold+1}.h5')\n","\n","    # infer valid data\n","\n","    p = model.predict(x_valid, batch_size=512, verbose=verbose).flatten()\n","\n","    print()\n","    print('fold {fold+1} cv = '.format(amex_metric_mod(y_valid,p)))\n","    print()\n","\n","    true = np.concatenate([true, y_valid])\n","    oof =  np.concatenate([oof,p])\n","\n","    # clean memory\n","    del model, x_trian, y_train, y_valid, x_valid, p\n","    gc.collect()\n","\n","  print('='*25)\n","\n","  "],"metadata":{"id":"pMuFUKUcmwQv"},"execution_count":null,"outputs":[]}]}