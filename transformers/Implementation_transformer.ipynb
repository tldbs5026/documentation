{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWs8FdMuOkkpf2Y2kHG28R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"9X_LOcjCXXWu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PO2b_kngXUAj"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow import layers"]},{"cell_type":"markdown","source":["#Implement a Transformer block as a layer"],"metadata":{"id":"WkKSW9QmXeO9"}},{"cell_type":"code","source":["class TransformerBlock(layers.Layer) :\n","  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1) :\n","    super().__init__()\n","    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","        [layers.Dense(ff_dim, activation='relu'),\n","         layers.Dense(embed_dim)]\n","    )\n","    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","\n","  def call(self, inputs, training) :\n","    attn_output = self.att(inputs, inputs)\n","    attn_output = self.dropout1(attn_output, training=training)\n","\n","    out1 = self.layernorm1(inputs + attn_output)\n","    \n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout(ffn_output, training=training)\n","    return self.layernorm2(out1 + ffn_output)"],"metadata":{"id":"fO-Eoq2kXdmw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Implement embedding layer\n","\n","Two seperate embedding layers, one for tokens, one for token index(positional)"],"metadata":{"id":"IdJ-pOUOZH7_"}},{"cell_type":"code","source":["class TokenAndPositionalEmbedding(layers.Layer) :\n","  def __init__(self, maxlen, vocab_size, embed_dim) :\n","    sueper().__init__()\n","    self.token_emb = layers.Embedding(input_dim = vocab_size, output_dim=embed_dim)\n","    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self,x) :\n","    maxlen = tf.shape(x)[-1]\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","\n","    x= self.token_emb(x)\n","    return x+positions"],"metadata":{"id":"4z2cz9EMZOIp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download and prepare dataset"],"metadata":{"id":"cEfhsrhgZv9U"}},{"cell_type":"code","source":["vocab_size = 20000\n","maxlen = 200\n","(x_train, y_train), (x_val, y_val) = keras.dataset.imdb.load_data(num_words = vocab_size)\n","\n","print(len(x_train), 'training sequences')\n","print(len(x_val), 'validation sequences')\n","x_train= keras.preprocessing.sequence.pad_sequence(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequence(x_val, maxlen=maxlen)"],"metadata":{"id":"8VaN_mSHZyPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Create classifier model using transformer layer\n","\n","Transformer layer outputs one vector for each time step of our input seuqence."],"metadata":{"id":"jwJiWLybaMyf"}},{"cell_type":"code","source":["embed_dim = 32  ## Embedding size for each token\n","num_heads = 2   ## #attention heads\n","ff_dim = 32     ## hidden layer size in feed forward network inside transformer\n","\n","\n","intputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","\n","transformer_block = TransformerBlock(embed_dim, num_hedas, ff_dim)\n","\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = Dense(20, activation='relu')(x)\n","x = layers.Dropout(0.1)(x)\n","\n","outputs = layers.Dense(2, activation='softmax')(x)\n","\n","model = keras.Model(inputs=inputs, outputs=ouputs)"],"metadata":{"id":"VRIBraeMaT1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train and Evaluate"],"metadata":{"id":"wa95Nkp4bCv7"}},{"cell_type":"code","source":["model = compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data = (x_val, y_val))"],"metadata":{"id":"HM3VKyx5bCIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HW_E4yS6bR-8"},"execution_count":null,"outputs":[]}]}